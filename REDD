python DATM.py --cfg ../configs/MNIST/ConvIN/IPC1.yaml

cd buffer
python buffer_FTD.py --dataset=MNIST --model=ConvNet --train_epochs=10 --num_experts=20 --zca --buffer_path=../buffer_storage/ --data_path=../dataset/ --rho_max=0.01 --rho_min=0.01 --alpha=0.3 --lr_teacher=0.01 --mom=0. --batch_train=256


(distillation) stevenubuntu@stevenubuntu:~/Documents/DATM-main/distill$ python DATM.py --cfg ../configs/MNIST/ConvIN/IPC10.yaml
True
CUDNN STATUS: True
Train ZCA
100%|██████████████████████████████████| 60000/60000 [00:01<00:00, 48531.29it/s]
Test ZCA
100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 50044.85it/s]
<class 'kornia.enhance.zca.ZCAWhitening'>
wandb: Currently logged in as: 15121204 (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.11
wandb: Run data is saved locally in /home/stevenubuntu/Documents/DATM-main/distill/wandb/run-20231205_015829-2hhpdm7m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-darkness-19
wandb: ⭐️ View project at https://wandb.ai/15121204/MNIST_ipc10
wandb: 🚀 View run at https://wandb.ai/15121204/MNIST_ipc10/runs/2hhpdm7m
Hyper-parameters: 
  0%|                                                 | 0/60000 [00:00<?, ?it/s] {'cfg': '../configs/MNIST/ConvIN/IPC10.yaml', 'dataset': 'MNIST', 'subset': 'imagenette', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_eval': 1, 'eval_it': 500, 'epoch_eval_train': 1000, 'Iteration': 100, 'lr_img': 100, 'lr_teacher': 0.01, 'lr_init': 0.01, 'batch_real': 256, 'batch_syn': 500, 'batch_train': 128, 'pix_init': 'samples_predicted_correctly', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '../dataset', 'buffer_path': '../buffer_storage/', 'expert_epochs': 2, 'syn_steps': 50, 'max_start_epoch': 20, 'min_start_epoch': 0, 'zca': True, 'load_all': False, 'no_aug': False, 'texture': False, 'canvas_size': 2, 'canvas_samples': 1, 'max_files': None, 'max_experts': None, 'force_save': False, 'ema_decay': 0.9995, 'lr_y': 2.0, 'Momentum_y': 0.9, 'project': 'MNIST_ipc10', 'threshold': 1.0, 'record_loss': False, 'Sequential_Generation': True, 'expansion_end_epoch': 1000, 'current_max_start_epoch': 10, 'skip_first_eva': True, 'parall_eva': False, 'lr_lr': 1e-05, 'res': 32, 'device': 'cuda', 'Initialize_Label_With_Another_Model': False, 'Initialize_Label_Model': '', 'Initialize_Label_Model_Dir': '', 'Label_Model_Timestamp': -1, 'zca_trans': ZCAWhitening(), 'im_size': [28, 28], 'dc_aug_param': None, 'dsa_param': <utils.utils_baseline.ParamDiffAug object at 0x7f24142281f0>, '_wandb': {}, 'distributed': False}
Evaluation model pool:  ['ConvNet']
BUILDING DATASET
/home/stevenubuntu/Documents/DATM-main/distill/DATM.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels_all.append(class_map[torch.tensor(sample[1]).item()])
100%|█████████████████████████████████| 60000/60000 [00:00<00:00, 249029.24it/s]
60000it [00:00, 2749642.06it/s]
class c = 0: 5923 real images
class c = 1: 6742 real images
class c = 2: 5958 real images
class c = 3: 6131 real images
class c = 4: 5842 real images
class c = 5: 5421 real images
class c = 6: 5918 real images
class c = 7: 6265 real images
class c = 8: 5851 real images
class c = 9: 5949 real images
real images channel 0, mean = -0.0000, std = 0.3602
/home/stevenubuntu/Documents/DATM-main/distill/DATM.py:133: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/utils/tensor_new.cpp:210.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
Expert Dir: ../buffer_storage/MNIST/ConvNet
loading file ../buffer_storage/MNIST/ConvNet/replay_buffer_2.pt
0.0.0
1.0.0
2.0.0
3.0.0
4.0.0
5.0.0
6.0.0
7.0.0
8.0.0
9.0.0
[2023-12-05 01:58:30] training begins
InitialAcc:1.0
/home/stevenubuntu/anaconda3/envs/distillation/lib/python3.9/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/TensorShape.cpp:2228.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[2023-12-05 01:58:31] iter = 0000, loss = 0.8683
loading file ../buffer_storage/MNIST/ConvNet/replay_buffer_1.pt
[2023-12-05 01:58:41] iter = 0010, loss = 0.9720
loading file ../buffer_storage/MNIST/ConvNet/replay_buffer_0.pt
[2023-12-05 01:58:50] iter = 0020, loss = 0.7261
loading file ../buffer_storage/MNIST/ConvNet/replay_buffer_4.pt
[2023-12-05 01:58:59] iter = 0030, loss = 0.9329
Traceback (most recent call last):
  File "/home/stevenubuntu/Documents/DATM-main/distill/DATM.py", line 592, in <module>
    main(args)
  File "/home/stevenubuntu/Documents/DATM-main/distill/DATM.py", line 494, in main
    target_params = expert_trajectory[start_epoch+args.expert_epochs]
IndexError: list index out of range

wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   Grand_Loss ▆▇▂█▇▆▅▄▁▁█▇▅▅▇▆▄▅▅▅▄▃▆▂▃▆▄▇▁▄▇▂
wandb:     Progress ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███
wandb:  Start_Epoch ▃▅▁█▅▅▃▂▁▁██▄▅█▆▃▅▅▅▃▃▇▂▃▇▅█▁▅█▃
wandb: Synthetic_LR ▁▁▁▂▂▂▂▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇████▇▇
wandb: 
wandb: Run summary:
wandb:   Grand_Loss 0.65648
wandb:     Progress 32
wandb:  Start_Epoch 2
wandb: Synthetic_LR 0.01374


